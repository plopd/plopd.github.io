<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mind in the Machine</title>
    <link>https://plopd.github.io/</link>
    <description>Recent content on Mind in the Machine</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://plopd.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Some Notes on Recent Developments of Emphatic Temporal Differences Methods and Emphatic Weightings</title>
      <link>https://plopd.github.io/post/emphatic/</link>
      <pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/emphatic/</guid>
      <description>I have recently read some of the work that has been done on emphatic temporal-differences (ETD) methods, and more generally on the use of emphatic weightings. Below I briefly summarize my understanding of the authors&amp;rsquo; findings, and offer some of my own opinions where I think they are appropriate.
ETD methods have been proposed to address the long-standing problem of instability of off-policy temporal-differences (TD) methods under linear function approximation (Sutton, Mahmood, and White 2016 1).</description>
    </item>
    
    <item>
      <title>Reward-respecting subtasks</title>
      <link>https://plopd.github.io/post/02-paper-distill/respect-reward/</link>
      <pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/02-paper-distill/respect-reward/</guid>
      <description>Reward-Respecting Subtasks for Model-Based Reinforcement Learning, 2022 1 TL;DR: Subtasks that are defined to explicitly serve the main reward signal lead to more efficient planning. Overview This paper focuses on demonstrating the effectiveness of (main) reward-respecting subtasks2 for better planning. It is well known that for an agent to act effectively in its environment, it should not only learn directly from experience, but also strive to learn a model of the environment and the possible consequences of its actions.</description>
    </item>
    
    <item>
      <title>Readlist</title>
      <link>https://plopd.github.io/booklist/readlist/</link>
      <pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/booklist/readlist/</guid>
      <description>Currently reading Thinking, Fast and Slow &amp;ndash; Daniel Kahneman Thinking in Systems: A Primer &amp;ndash; Donella H. Meadows, Diana Wright A few reads I really enjoyed Scale &amp;ndash; Geoffrey West Where Is My Flying Car? &amp;ndash; J. Storrs Hall The Future of Humanity &amp;ndash; Michio Kaku </description>
    </item>
    
    <item>
      <title>ViT</title>
      <link>https://plopd.github.io/post/02-paper-distill/vit/</link>
      <pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/02-paper-distill/vit/</guid>
      <description>Vision Transformers (ViT), 2020 Overview This week I took a closer look at the ViT paper which contains some interesting experiments on how learning in the Transformer model scales with increasing computer power and data 1.
The paper experiments with applying the Transformer model to images, making as few changes as possible to the original architecture. The authors are interested in whether a Transformer can learn more computationally efficient inductive biases that might otherwise be designed into the network architecture itself.</description>
    </item>
    
    <item>
      <title>A Simple Case of Supervised Online Learning</title>
      <link>https://plopd.github.io/post/01-online-rl/online-learning/</link>
      <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/01-online-rl/online-learning/</guid>
      <description>In this paper, we consider the supervised version of online learning. Specifically, we try to predict the label of handwritten digits, from the MNIST dataset, in images in an online fashion. The samples arrive one by one, and the training is performed immediately afterwards.
We evaluate three online methods, namely
Incremental: the learner performs a single learning update at each time step in a fully incremental manner, with no samples stored.</description>
    </item>
    
    <item>
      <title>On the Importance of the Agent-Environment Interaction for Better Understanding the World</title>
      <link>https://plopd.github.io/research/thoughts-on-data/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/research/thoughts-on-data/</guid>
      <description>Recent developments in natural language processing (NLP) &amp;amp; natural language understanding (NLU) 1 and image generation 2 have generated a lot of excitement and well-deserved enthusiasm. But&amp;hellip;
Although people claim that these models learn by self-monitoring, they actually learn from human labeled data, which makes them supervised learning methods. Why is that?
Let&amp;rsquo;s take the example of GPT-3, which learns from a huge corpus of text from the Internet. The model learns to predict the next word in sequence by recycling this corpus many times, and after some time it can accurately predict the next word, so it seems to have gained understanding and knowledge about the world.</description>
    </item>
    
    <item>
      <title>&#39;When is Prediction Knowledge?&#39; paper</title>
      <link>https://plopd.github.io/research/predictions/</link>
      <pubDate>Sun, 06 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/research/predictions/</guid>
      <description>Notes The motivation given in the paper 1 for representing machine knowledge as predictions is to give the agent the ability to construct its own world knowledge of the world, and not to depend on an external designer.
If we consider that the only thing an agent can control is its own stream of experience coming in from the world, then knowledge can be constructed from sensations about that stream, the agent&amp;rsquo;s past and subsequent behavior of having taken an action, and time.</description>
    </item>
    
    <item>
      <title>Gato</title>
      <link>https://plopd.github.io/research/gato/</link>
      <pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/research/gato/</guid>
      <description>Notes The agent is trained using supervised learning, not reinforcement learning:
For simplicity, Gato was trained offline in a purely supervised fashion; however, in principle, there is no reason why it could not be trained with either offline or online reinforcement learning (RL).
And it does require some &amp;ldquo;divine intervention&amp;rdquo; after it has finished training and is deployed to tackle new tasks:
To adapt the agent to new tasks or behaviors, we choose to fine-tune the agent&amp;rsquo;s parameters on a limited number of demonstrations of a single task, and then evaluate the fine-tuned model&amp;rsquo;s performance in the environment.</description>
    </item>
    
  </channel>
</rss>
