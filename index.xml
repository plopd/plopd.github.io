<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mind in the Machine</title>
    <link>https://plopd.github.io/</link>
    <description>Recent content on Mind in the Machine</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 17 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://plopd.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://plopd.github.io/booklist/fastslow/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/booklist/fastslow/</guid>
      <description>It is incredible that, even with the best of intentions, we may not realise the fallacies of our thinking until we carefully review them afterwards &amp;ndash; emotions &amp;ldquo;interfere&amp;rdquo;. System 1 (fast and intuitive) and System 2 (slow and meticulous) are part of the human mind. But how did they form? What was in the beginning? How should we think about these two systems when we design the computational mind? Is it the case that they are part of a whole, with more of System 1 in the beginning and more computational resources gradually being allocated to System 2?</description>
    </item>
    
    <item>
      <title>Reinitializing the moments of the optimizer during learning (Jax)</title>
      <link>https://plopd.github.io/post/jax/reinit_moments_opt/</link>
      <pubDate>Sat, 27 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/jax/reinit_moments_opt/</guid>
      <description>Reinitializing the weights of a neural network has been shown to be effective for continual learning (see the CBP algorithm 1). In this short tutorial I show how to update the moments of a momentum-based optimizer in Jax. Changing the optimizer&amp;rsquo;s state is a basic step towards a full specification of the CBP algorithm for momentum-based optimizers. Here we reinitialize all components of the two moments, not just those corresponding to low utility features (i.</description>
    </item>
    
    <item>
      <title>Accumulating values at specific indices (NumPy)</title>
      <link>https://plopd.github.io/post/np/acc_grads/</link>
      <pubDate>Fri, 12 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/np/acc_grads/</guid>
      <description>I came across np.add.at recently and found out that it is a very powerful NumPy method if you want to accumulate values at specific indices. Consider the following example, where you want to embed some inputs in a D-dimensional vector space and compute the gradients of the embedding weight matrix:
... dW = np.zeros((C, D)) # tensor storing the gradients of the embedding matrix `W`. dout.shape = (N, T, D) # upstream gradients of some embedded input sequences.</description>
    </item>
    
    <item>
      <title>Notes on Some Recent Developments in Emphatic Temporal-Differences Methods and Emphatic Weightings</title>
      <link>https://plopd.github.io/post/emphatic/</link>
      <pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/emphatic/</guid>
      <description>I have recently read some of the work that has been done on emphatic temporal-differences (ETD) methods, and more generally on the use of emphatic weightings. Below I briefly summarize my understanding of the authors&amp;rsquo; findings, and offer some of my own opinions where I think they are appropriate.
ETD methods have been proposed to address the long-standing problem of instability of off-policy temporal-differences (TD) methods under linear function approximation (Sutton, Mahmood, and White 2016 1).</description>
    </item>
    
    <item>
      <title>Reward-respecting subtasks</title>
      <link>https://plopd.github.io/post/02-paper-distill/respect-reward/</link>
      <pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/02-paper-distill/respect-reward/</guid>
      <description>Reward-Respecting Subtasks for Model-Based Reinforcement Learning, 2022 1 TL;DR: Subtasks that are defined to explicitly serve the main reward signal lead to more efficient planning. Overview This paper focuses on demonstrating the effectiveness of (main) reward-respecting subtasks2 for better planning. It is well known that for an agent to act effectively in its environment, it should not only learn directly from experience, but also strive to learn a model of the environment and the possible consequences of its actions.</description>
    </item>
    
    <item>
      <title>Readlist</title>
      <link>https://plopd.github.io/booklist/readlist/</link>
      <pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/booklist/readlist/</guid>
      <description>Currently reading The Computer and the Brain (Third Edition) &amp;ndash; John von Neumann Thinking, Fast and Slow &amp;ndash; Daniel Kahneman Thinking in Systems: A Primer &amp;ndash; Donella H. Meadows, Diana Wright A few reads I really enjoyed Scale &amp;ndash; Geoffrey West Where Is My Flying Car? &amp;ndash; J. Storrs Hall The Future of Humanity &amp;ndash; Michio Kaku </description>
    </item>
    
    <item>
      <title>ViT</title>
      <link>https://plopd.github.io/post/02-paper-distill/vit/</link>
      <pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/02-paper-distill/vit/</guid>
      <description>Vision Transformers (ViT), 2020 Overview This week I took a closer look at the ViT paper which contains some interesting experiments on how learning in the Transformer model scales with increasing computer power and data 1.
The paper experiments with applying the Transformer model to images, making as few changes as possible to the original architecture. The authors are interested in whether a Transformer can learn more computationally efficient inductive biases that might otherwise be designed into the network architecture itself.</description>
    </item>
    
    <item>
      <title>A Simple Case of Supervised Online Learning</title>
      <link>https://plopd.github.io/post/01-online-rl/online-learning/</link>
      <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/01-online-rl/online-learning/</guid>
      <description>In this paper, we consider the supervised version of online learning. Specifically, we try to predict the label of handwritten digits, from the MNIST dataset, in images in an online fashion. The samples arrive one by one, and the training is performed immediately afterwards.
We evaluate three online methods, namely
Incremental: the learner performs a single learning update at each time step in a fully incremental manner, with no samples stored.</description>
    </item>
    
    <item>
      <title>On the Importance of the Agent-Environment Interaction for Better Understanding the World</title>
      <link>https://plopd.github.io/research/thoughts-on-data/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/research/thoughts-on-data/</guid>
      <description>Recent developments in natural language processing (NLP) &amp;amp; natural language understanding (NLU) 1 and image generation 2 have generated a lot of excitement and well-deserved enthusiasm. But&amp;hellip;
Although people claim that these models learn by self-monitoring, they actually learn from human labeled data, which makes them supervised learning methods. Why is that?
Let&amp;rsquo;s take the example of GPT-3, which learns from a huge corpus of text from the Internet. The model learns to predict the next word in sequence by recycling this corpus many times, and after some time it can accurately predict the next word, so it seems to have gained understanding and knowledge about the world.</description>
    </item>
    
    <item>
      <title>&#39;When is Prediction Knowledge?&#39; paper</title>
      <link>https://plopd.github.io/research/predictions/</link>
      <pubDate>Sun, 06 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/research/predictions/</guid>
      <description>Notes The motivation given in the paper 1 for representing machine knowledge as predictions is to give the agent the ability to construct its own world knowledge of the world, and not to depend on an external designer.
If we consider that the only thing an agent can control is its own stream of experience coming in from the world, then knowledge can be constructed from sensations about that stream, the agent&amp;rsquo;s past and subsequent behavior of having taken an action, and time.</description>
    </item>
    
    <item>
      <title>Gato</title>
      <link>https://plopd.github.io/research/gato/</link>
      <pubDate>Thu, 07 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/research/gato/</guid>
      <description>Notes The agent is trained using supervised learning, not reinforcement learning:
For simplicity, Gato was trained offline in a purely supervised fashion; however, in principle, there is no reason why it could not be trained with either offline or online reinforcement learning (RL).
And it does require some &amp;ldquo;divine intervention&amp;rdquo; after it has finished training and is deployed to tackle new tasks:
To adapt the agent to new tasks or behaviors, we choose to fine-tune the agent&amp;rsquo;s parameters on a limited number of demonstrations of a single task, and then evaluate the fine-tuned model&amp;rsquo;s performance in the environment.</description>
    </item>
    
  </channel>
</rss>
