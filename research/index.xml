<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Researches on Mind in the Machine</title>
    <link>https://plopd.github.io/research/</link>
    <description>Recent content in Researches on Mind in the Machine</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://plopd.github.io/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>On the Importance of the Agent-Environment Interaction for Better Understanding the World</title>
      <link>https://plopd.github.io/research/thoughts-on-data/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/research/thoughts-on-data/</guid>
      <description>Recent developments in natural language processing (NLP) &amp;amp; natural language understanding (NLU) 1 and image generation 2 have generated a lot of excitement and well-deserved enthusiasm. But&amp;hellip;
Although people claim that these models learn by self-monitoring, they actually learn from human labeled data, which makes them supervised learning methods. Why is that?
Let&amp;rsquo;s take the example of GPT-3, which learns from a huge corpus of text from the Internet. The model learns to predict the next word in sequence by recycling this corpus many times, and after some time it can accurately predict the next word, so it seems to have gained understanding and knowledge about the world.</description>
    </item>
    
  </channel>
</rss>
