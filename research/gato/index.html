<!doctype html>
<html lang="en"><head>
    <title>Gato</title>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="" />
    

    
    
    
    <link rel="stylesheet" href="../../css/theme.min.css">

    
    
    
    
    <link rel="stylesheet" href="../../css/custom.min.css">
    

    
</head>
<body>
        <div id="content" class="mx-auto"><header class="container mt-sm-5 mt-4 mb-4 mt-xs-1">
    <div class="row">
        
        <div class="col-sm-8 col-12 text-sm-left text-center">
            
            
            <ul id="nav-links" class="list-inline mb-2">
                
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../" title="About">About</a>
                    </li>
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../post/" title="Blog">Blog</a>
                    </li>
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../booklist/readlist/" title="Reading List">Reading List</a>
                    </li>
                
            </ul>
            <ul id="nav-social" class="list-inline">
                
            </ul>
        </div>
    </div>
    <hr />
</header>
<div class="container">
    <div class="pl-sm-3 ml-sm-5">
        <h3 id="notes">Notes</h3>
<p>The agent is trained using supervised learning, not reinforcement learning:</p>
<blockquote>
<p>For simplicity, Gato was trained offline in a purely supervised fashion; however, in principle, there is no reason why it could not be trained with either offline or online reinforcement learning (RL).</p>
</blockquote>
<p>And it does require some &ldquo;divine intervention&rdquo; <em>after</em> it has finished training and is deployed to tackle new tasks:</p>
<blockquote>
<p>To adapt the agent to new tasks or behaviors, we choose to fine-tune the agent&rsquo;s parameters on a limited number of demonstrations of a single task, and then evaluate the fine-tuned model&rsquo;s performance in the environment.</p>
</blockquote>
<p>I like parts of the hypothesis, in particular</p>
<blockquote>
<p>It is possible to train an agent that is generally competent on a large number of tasks;</p>
</blockquote>
<p>In particular, the emphasis on developing agents that can solve tasks, rather than aiming for superhuman performance on those tasks. This way of thinking leaves much more room for the exploration of continuous learning capabilities. In the limit of ever-increasing computational and &ldquo;training&rdquo; time, if the agent can learn continuously over its lifetime, it is likely to surpass human-level performance in many, if not all, tasks.</p>
<p>A novel aspect of the paper, in my opinion, was the tokenization of various modalities into a flat sequence that is fed as input to a transformer. What I would definitely like to see in future work is to significantly reduce the number of tasks, and to go beyond tokenization to trying to learn from a <em>raw</em> bitstream (as attempted in <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>).</p>
<p>Another limitation of the current work, and thus an opportunity for improvement, is that the</p>
<blockquote>
<p>&hellip; agent observations are not currently predicted in Gato.</p>
</blockquote>
<p>Predicting the next observations from the world is critical for a self-learning agent, because in order to build knowledge about the world, it must check for itself beforehand whether that knowledge is useful or not <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>I am not a big fan of supervised learning because it does not scale, but frankly there is something special about training an agent by supervised learning from the experience of other <em>specialized RL</em> agents.</p>
<blockquote>
<p>We found it effective to train on a filtered set of episodes with yields at least 80% of the expert yield for the task.</p>
</blockquote>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Jaegle, A., Borgeaud, S., Alayrac, J. B., Doersch, C., Ionescu, C., Ding, D., &hellip; &amp; Carreira, J. (2021). Perceiver io: A general architecture for structured inputs &amp; outputs. arXiv preprint arXiv:2107.14795.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Sutton, R. S. (2011, July). Beyond reward: The problem of knowledge and data. In International Conference on Inductive Logic Programming (pp. 2-6). Springer, Berlin, Heidelberg.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    </div>

            </div>
        </div><footer class="text-center pb-1">
    <small class="text-muted">
        
        <br>
        Built with <a href="https://gohugo.io/" target="_blank">Hugo</a>
        and <a href="https://github.com/austingebauer/devise" target="_blank">Devise</a>
        theme from <a href="https://github.com/austingebauer/devise" target="_blank">A. Gebauer.</a>
    </small>
</footer>
</body>
</html>
