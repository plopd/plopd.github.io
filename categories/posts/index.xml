<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>posts on Mind in the Machine</title>
    <link>https://plopd.github.io/categories/posts/</link>
    <description>Recent content in posts on Mind in the Machine</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 27 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://plopd.github.io/categories/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reinitializing the moments of the optimizer during learning (Jax)</title>
      <link>https://plopd.github.io/post/jax/reinit_moments_opt/</link>
      <pubDate>Sat, 27 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/jax/reinit_moments_opt/</guid>
      <description>Reinitializing the weights of a neural network has been shown to be effective for continual learning (see the CBP algorithm 1). In this short tutorial I show how to update the moments of a momentum-based optimizer in Jax. Changing the optimizer&amp;rsquo;s state is a basic step towards a full specification of the CBP algorithm for momentum-based optimizers. Here we reinitialize all components of the two moments, not just those corresponding to low utility features (i.</description>
    </item>
    
    <item>
      <title>Accumulating values at specific indices (NumPy)</title>
      <link>https://plopd.github.io/post/np/acc_grads/</link>
      <pubDate>Fri, 12 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/np/acc_grads/</guid>
      <description>I came across np.add.at recently and found out that it is a very powerful NumPy method if you want to accumulate values at specific indices. Consider the following example, where you want to embed some inputs in a D-dimensional vector space and compute the gradients of the embedding weight matrix:
... dW = np.zeros((C, D)) # tensor storing the gradients of the embedding matrix `W`. dout.shape = (N, T, D) # upstream gradients of some embedded input sequences.</description>
    </item>
    
    <item>
      <title>Notes on Some Recent Developments in Emphatic Temporal-Differences Methods and Emphatic Weightings</title>
      <link>https://plopd.github.io/post/emphatic/</link>
      <pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/emphatic/</guid>
      <description>I have recently read some of the work that has been done on emphatic temporal-differences (ETD) methods, and more generally on the use of emphatic weightings. Below I briefly summarize my understanding of the authors&amp;rsquo; findings, and offer some of my own opinions where I think they are appropriate.
ETD methods have been proposed to address the long-standing problem of instability of off-policy temporal-differences (TD) methods under linear function approximation (Sutton, Mahmood, and White 2016 1).</description>
    </item>
    
    <item>
      <title>Reward-respecting subtasks (notes)</title>
      <link>https://plopd.github.io/post/02-paper-distill/respect-reward/</link>
      <pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/02-paper-distill/respect-reward/</guid>
      <description>In this post, I review the paper Reward-respecting subtasks for model-based reinforcement learning by Sutton, R. S., Machado, M. C., Holland, G. Z., Timbers, D. S. F., Tanner, B., &amp;amp; White, A. (2022) 1.
TL;DR: Subtasks that are defined to explicitly serve the main reward signal lead to more efficient planning. Overview This paper focuses on demonstrating the effectiveness of (main) reward-respecting subtasks2 for better planning. It is well known that for an agent to act effectively in its environment, it should not only learn directly from experience, but also strive to learn a model of the environment and the possible consequences of its actions.</description>
    </item>
    
    <item>
      <title>ViT</title>
      <link>https://plopd.github.io/post/02-paper-distill/vit/</link>
      <pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/02-paper-distill/vit/</guid>
      <description>Vision Transformers (ViT), 2020 Overview This week I took a closer look at the ViT paper which contains some interesting experiments on how learning in the Transformer model scales with increasing computer power and data 1.
The paper experiments with applying the Transformer model to images, making as few changes as possible to the original architecture. The authors are interested in whether a Transformer can learn more computationally efficient inductive biases that might otherwise be designed into the network architecture itself.</description>
    </item>
    
    <item>
      <title>A Simple Case of Supervised Online Learning</title>
      <link>https://plopd.github.io/post/01-online-rl/online-learning/</link>
      <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/01-online-rl/online-learning/</guid>
      <description>In this paper, we consider the supervised version of online learning. Specifically, we try to predict the label of handwritten digits, from the MNIST dataset, in images in an online fashion. The samples arrive one by one, and the training is performed immediately afterwards.
We evaluate three online methods, namely
Incremental: the learner performs a single learning update at each time step in a fully incremental manner, with no samples stored.</description>
    </item>
    
    <item>
      <title>On the Importance of the Agent-Environment Interaction for Better Understanding the World</title>
      <link>https://plopd.github.io/research/thoughts-on-data/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/research/thoughts-on-data/</guid>
      <description>Recent developments in natural language processing (NLP) &amp;amp; natural language understanding (NLU) 1 and image generation 2 have generated a lot of excitement and well-deserved enthusiasm. But&amp;hellip;
Although people claim that these models learn by self-monitoring, they actually learn from human labeled data, which makes them supervised learning methods. Why is that?
Let&amp;rsquo;s take the example of GPT-3, which learns from a huge corpus of text from the Internet. The model learns to predict the next word in sequence by recycling this corpus many times, and after some time it can accurately predict the next word, so it seems to have gained understanding and knowledge about the world.</description>
    </item>
    
  </channel>
</rss>
