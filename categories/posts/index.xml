<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>posts on Mind in the Machine</title>
    <link>https://plopd.github.io/categories/posts/</link>
    <description>Recent content in posts on Mind in the Machine</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 27 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://plopd.github.io/categories/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reinitializing the moments of the optimizer during learning (Jax)</title>
      <link>https://plopd.github.io/post/jax/reinit_moments_opt/</link>
      <pubDate>Sat, 27 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/jax/reinit_moments_opt/</guid>
      <description>Reinitializing the weights of a neural network has been shown to be effective for continual learning (see the CBP algorithm 1). In this short tutorial I show how to update the moments of a momentum-based optimizer in Jax. Changing the optimizer&amp;rsquo;s state is a basic step towards a full specification of the CBP algorithm for momentum-based optimizers. Here we reinitialize all components of the two moments, not just those corresponding to low utility features (i.</description>
    </item>
    
    <item>
      <title>Accumulating values at specific indices (NumPy)</title>
      <link>https://plopd.github.io/post/np/acc_grads/</link>
      <pubDate>Fri, 12 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/np/acc_grads/</guid>
      <description>I came across np.add.at recently and found out that it is a very powerful NumPy method if you want to accumulate values at specific indices. Consider the following example, where you want to embed some inputs in a D-dimensional vector space and compute the gradients of the embedding weight matrix:
... dW = np.zeros((C, D)) # tensor storing the gradients of the embedding matrix `W`. dout.shape = (N, T, D) # upstream gradients of some embedded input sequences.</description>
    </item>
    
    <item>
      <title>Notes on Some Recent Developments in Emphatic Temporal-Differences Methods and Emphatic Weightings</title>
      <link>https://plopd.github.io/post/emphatic/</link>
      <pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/emphatic/</guid>
      <description>I have recently read some of the work that has been done on emphatic temporal-differences (ETD) methods, and more generally on the use of emphatic weightings. Below I briefly summarize my understanding of the authors&amp;rsquo; findings, and offer some of my own opinions where I think they are appropriate.
ETD methods have been proposed to address the long-standing problem of instability of off-policy temporal-differences (TD) methods under linear function approximation (Sutton, Mahmood, and White 2016 1).</description>
    </item>
    
    <item>
      <title>ViT</title>
      <link>https://plopd.github.io/post/02-paper-distill/vit/</link>
      <pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://plopd.github.io/post/02-paper-distill/vit/</guid>
      <description>Vision Transformers (ViT), 2020 Overview This week I took a closer look at the ViT paper which contains some interesting experiments on how learning in the Transformer model scales with increasing computer power and data 1.
The paper experiments with applying the Transformer model to images, making as few changes as possible to the original architecture. The authors are interested in whether a Transformer can learn more computationally efficient inductive biases that might otherwise be designed into the network architecture itself.</description>
    </item>
    
  </channel>
</rss>
